{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1caf1691-1415-4c24-a845-255c664ec202",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-20T14:53:45.282739Z",
     "iopub.status.busy": "2024-02-20T14:53:45.282334Z",
     "iopub.status.idle": "2024-02-20T14:54:07.687706Z",
     "shell.execute_reply": "2024-02-20T14:54:07.685996Z",
     "shell.execute_reply.started": "2024-02-20T14:53:45.282696Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb3a0d4-67aa-4ea6-b8c2-40dc14052157",
   "metadata": {},
   "source": [
    "## FLAN T5 prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d05068e-69f9-4d51-9d5c-65410b0c60a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-20T14:54:07.701602Z",
     "iopub.status.busy": "2024-02-20T14:54:07.701186Z",
     "iopub.status.idle": "2024-02-20T14:54:12.747915Z",
     "shell.execute_reply": "2024-02-20T14:54:12.745810Z",
     "shell.execute_reply.started": "2024-02-20T14:54:07.701557Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/hoannc8/envs/torch_py310/lib/python3.10/site-packages/transformers/generation/utils.py:1133: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Wie ich er bitten?</s>\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "device = \"cuda:2\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=device)\n",
    "\n",
    "input_text = \"translate English to German: How old are you?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c631a2-0560-4460-8b8b-e6e9e70dd9c5",
   "metadata": {},
   "source": [
    "## Zero shot sentiment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35f218c8-29bb-4c66-b882-4d3973e3758e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-20T14:54:12.838832Z",
     "iopub.status.busy": "2024-02-20T14:54:12.838026Z",
     "iopub.status.idle": "2024-02-20T14:54:12.944129Z",
     "shell.execute_reply": "2024-02-20T14:54:12.941035Z",
     "shell.execute_reply.started": "2024-02-20T14:54:12.838777Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Please classify the sentiment of the following statement as 'positive' or 'negative':\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruct_promt = \"Please classify the sentiment of the following statement as 'positive' or 'negative':\"\n",
    "instruct_promt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3304991-13e4-41b8-b70d-919a411b7275",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-20T14:54:12.948741Z",
     "iopub.status.busy": "2024-02-20T14:54:12.948329Z",
     "iopub.status.idle": "2024-02-20T14:54:13.046201Z",
     "shell.execute_reply": "2024-02-20T14:54:13.044193Z",
     "shell.execute_reply.started": "2024-02-20T14:54:12.948713Z"
    }
   },
   "outputs": [],
   "source": [
    "input_text = \"I hate this film. It's so boring!\"\n",
    "input_prompt = instruct_promt + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "419bcccb-a5e1-4060-8cb7-17ae841e2927",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-20T14:54:13.049354Z",
     "iopub.status.busy": "2024-02-20T14:54:13.048594Z",
     "iopub.status.idle": "2024-02-20T14:54:13.384377Z",
     "shell.execute_reply": "2024-02-20T14:54:13.383104Z",
     "shell.execute_reply.started": "2024-02-20T14:54:13.049289Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(input_prompt, return_tensors=\"pt\")\n",
    "inputs = inputs.to(device)\n",
    "outputs = model.generate(**inputs)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acf87e7-9572-454e-85bd-d6dfecbda879",
   "metadata": {},
   "source": [
    "### Working with IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb13c9ac-ce24-4c6b-a015-bc6633bff4e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-20T14:54:13.437031Z",
     "iopub.status.busy": "2024-02-20T14:54:13.436515Z",
     "iopub.status.idle": "2024-02-20T14:54:25.900673Z",
     "shell.execute_reply": "2024-02-20T14:54:25.898933Z",
     "shell.execute_reply.started": "2024-02-20T14:54:13.436980Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clich√©d and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.',\n",
       " 'labels': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\", split=\"test\")\n",
    "dataset = dataset.rename_column(\"label\", \"labels\") #match trainer column\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be419328-a7f9-4e9d-bc7c-4d53eb92acaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-20T14:54:25.902889Z",
     "iopub.status.busy": "2024-02-20T14:54:25.902385Z",
     "iopub.status.idle": "2024-02-20T14:54:25.998189Z",
     "shell.execute_reply": "2024-02-20T14:54:25.997167Z",
     "shell.execute_reply.started": "2024-02-20T14:54:25.902850Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(instruct_promt+dataset[0]['text'], return_tensors=\"pt\")\n",
    "inputs = inputs.to(device)\n",
    "outputs = model.generate(**inputs)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb0713a6-1860-4005-9869-f75698765114",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-20T14:54:25.999856Z",
     "iopub.status.busy": "2024-02-20T14:54:25.999554Z",
     "iopub.status.idle": "2024-02-20T14:54:26.005630Z",
     "shell.execute_reply": "2024-02-20T14:54:26.004517Z",
     "shell.execute_reply.started": "2024-02-20T14:54:25.999827Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer([instruct_promt+text for text in examples[\"text\"]],\n",
    "                     truncation=True,\n",
    "                     padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db03c345-df09-44ce-9977-e72e432c9b32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-20T14:54:26.007185Z",
     "iopub.status.busy": "2024-02-20T14:54:26.006899Z",
     "iopub.status.idle": "2024-02-20T14:55:02.455128Z",
     "shell.execute_reply": "2024-02-20T14:55:02.453615Z",
     "shell.execute_reply.started": "2024-02-20T14:54:26.007158Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892df8241e634f248dbd36954bc370a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, num_proc=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e37ea8f4-0270-484c-a80f-e6b29d112b8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-20T14:55:02.457546Z",
     "iopub.status.busy": "2024-02-20T14:55:02.457086Z",
     "iopub.status.idle": "2024-02-20T14:55:02.471036Z",
     "shell.execute_reply": "2024-02-20T14:55:02.469483Z",
     "shell.execute_reply.started": "2024-02-20T14:55:02.457499Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'labels', 'input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4269c93-e0e1-433b-b7f5-0b4489aca719",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-20T14:55:02.473328Z",
     "iopub.status.busy": "2024-02-20T14:55:02.472814Z",
     "iopub.status.idle": "2024-02-20T14:55:05.481822Z",
     "shell.execute_reply": "2024-02-20T14:55:05.480412Z",
     "shell.execute_reply.started": "2024-02-20T14:55:02.473279Z"
    }
   },
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"f1\")\n",
    "def compute_metrics(eval_pred):\n",
    "    print(eval_pred)\n",
    "    # predictions, labels = eval_pred\n",
    "    \n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    return metric.compute(predictions=predictions,\n",
    "                          references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2aea1e20-3844-4bb8-a451-5a90a813f566",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-20T14:55:05.486280Z",
     "iopub.status.busy": "2024-02-20T14:55:05.485249Z",
     "iopub.status.idle": "2024-02-20T14:55:09.009474Z",
     "shell.execute_reply": "2024-02-20T14:55:09.007662Z",
     "shell.execute_reply.started": "2024-02-20T14:55:05.486202Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# eval\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../results/promting_T5\",\n",
    "    per_device_eval_batch_size=64,\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    eval_dataset=tokenized_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f129856b-bd7e-43c5-998e-8b05b7c8a2b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-20T14:55:09.013470Z",
     "iopub.status.busy": "2024-02-20T14:55:09.012713Z",
     "iopub.status.idle": "2024-02-20T14:55:14.911732Z",
     "shell.execute_reply": "2024-02-20T14:55:14.910455Z",
     "shell.execute_reply.started": "2024-02-20T14:55:09.013417Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m evaluation_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/torch_py310/lib/python3.10/site-packages/transformers/trainer.py:3095\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3092\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3094\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3095\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3096\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3098\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3099\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3103\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3105\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/envs/torch_py310/lib/python3.10/site-packages/transformers/trainer.py:3284\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3281\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   3283\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 3284\u001b[0m loss, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3285\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3286\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/torch_py310/lib/python3.10/site-packages/transformers/trainer.py:3501\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   3499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_labels \u001b[38;5;129;01mor\u001b[39;00m loss_without_labels:\n\u001b[1;32m   3500\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3501\u001b[0m         loss, outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3502\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m   3504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/envs/torch_py310/lib/python3.10/site-packages/transformers/trainer.py:2795\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2793\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2794\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2795\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2796\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2797\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/envs/torch_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/torch_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/torch_py310/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1743\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1740\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1743\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1756\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1758\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1760\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/torch_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/torch_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/torch_py310/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1018\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1015\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to initialize the model with valid token embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1016\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens(input_ids)\n\u001b[0;32m-> 1018\u001b[0m batch_size, seq_length \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;66;03m# required mask seq length can be calculated via length of past\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m mask_seq_length \u001b[38;5;241m=\u001b[39m past_key_values[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m seq_length \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m seq_length\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "evaluation_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7821624-59dd-4d24-9995-f0e87ac2f99d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-20T14:55:14.912537Z",
     "iopub.status.idle": "2024-02-20T14:55:14.912815Z",
     "shell.execute_reply": "2024-02-20T14:55:14.912697Z",
     "shell.execute_reply.started": "2024-02-20T14:55:14.912684Z"
    }
   },
   "outputs": [],
   "source": [
    "def postprocess_text(predictions, labels):\n",
    "    predictions = [prediction.strip() for prediction in predictions]\n",
    "    labels = [label2id[label.strip()] for label in labels]\n",
    "\n",
    "    for idx in range(len(predictions)):\n",
    "        if predictions[idx] in label2id:\n",
    "           predictions[idx] = label2id[predictions[idx]]\n",
    "        else:\n",
    "            predictions[idx] = '-100'\n",
    "    return predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dd5438-0661-40da-badf-eb7674d72eba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce1c8b7-26e5-4faf-8f46-ce471a4ec3aa",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-20T14:55:14.913924Z",
     "iopub.status.idle": "2024-02-20T14:55:14.914174Z",
     "shell.execute_reply": "2024-02-20T14:55:14.914060Z",
     "shell.execute_reply.started": "2024-02-20T14:55:14.914042Z"
    }
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f84b9d-f68d-4eca-862e-c5aac6549529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8633be-3c93-439b-ad12-8257db34f963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a7b9f7-dcdd-4b33-ad1e-fc432da4915a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce76d7e-91a3-46fa-8958-b2086c5c78be",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-20T14:55:14.914933Z",
     "iopub.status.idle": "2024-02-20T14:55:14.915185Z",
     "shell.execute_reply": "2024-02-20T14:55:14.915069Z",
     "shell.execute_reply.started": "2024-02-20T14:55:14.915050Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_prompt(examples, top_k_indices, corpus, labels, id2label):\n",
    "    sentences = examples[\"text\"]\n",
    "    prompts = []\n",
    "    for index, _ in enumerate(zip(sentences)):\n",
    "        prompt = 'Here are examples of texts and their sentiments'\n",
    "        top_indexs = top_k_indices[index]['top_index']\n",
    "        for top_index in top_indexs:\n",
    "            top_sentence = corpus[top_index]\n",
    "            top_label = id2label[str(labels[top_index])]\n",
    "            prompt = \" \".join(\n",
    "                [\n",
    "                    prompt,\n",
    "                    \". Text: \", \n",
    "                    top_sentence,\n",
    "                    \". Sentiment: \",\n",
    "                    top_label\n",
    "                ]\n",
    "            )\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    examples[\"prompt\"] = prompts\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaea6350-f71e-4ed5-b1b5-40be992782ef",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-20T14:55:14.915896Z",
     "iopub.status.idle": "2024-02-20T14:55:14.916143Z",
     "shell.execute_reply": "2024-02-20T14:55:14.916025Z",
     "shell.execute_reply.started": "2024-02-20T14:55:14.916013Z"
    }
   },
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\n",
    "    \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
    ").input_ids.to(device)  # Batch size 1\n",
    "decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids.to(device)  # Batch size 1\n",
    "\n",
    "# preprocess: Prepend decoder_input_ids with start token which is pad token for T5Model.\n",
    "# This is not needed for torch's T5ForConditionalGeneration as it does this internally using labels arg.\n",
    "decoder_input_ids = model._shift_right(decoder_input_ids)\n",
    "\n",
    "# forward pass\n",
    "outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec6ffb5-6340-4969-a5c1-8097e0f20047",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-20T14:55:14.916927Z",
     "iopub.status.idle": "2024-02-20T14:55:14.917167Z",
     "shell.execute_reply": "2024-02-20T14:55:14.917051Z",
     "shell.execute_reply.started": "2024-02-20T14:55:14.917039Z"
    }
   },
   "outputs": [],
   "source": [
    "last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42b62db-9a85-48c9-a6a5-742ef3d830f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T02:09:15.987469Z",
     "iopub.status.busy": "2024-02-18T02:09:15.986727Z",
     "iopub.status.idle": "2024-02-18T02:09:16.015524Z",
     "shell.execute_reply": "2024-02-18T02:09:16.014042Z",
     "shell.execute_reply.started": "2024-02-18T02:09:15.987397Z"
    },
    "scrolled": true
   },
   "source": [
    "## Evaluate on IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467a9aa1-d879-42c0-a4ff-659158c22ff3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-20T14:55:14.917944Z",
     "iopub.status.idle": "2024-02-20T14:55:14.918190Z",
     "shell.execute_reply": "2024-02-20T14:55:14.918081Z",
     "shell.execute_reply.started": "2024-02-20T14:55:14.918069Z"
    }
   },
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"f1\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    return metric.compute(predictions=predictions,\n",
    "                          references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f540c18e-3420-4818-8cfa-2c106a6398e7",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-20T14:55:14.919470Z",
     "iopub.status.idle": "2024-02-20T14:55:14.919710Z",
     "shell.execute_reply": "2024-02-20T14:55:14.919601Z",
     "shell.execute_reply.started": "2024-02-20T14:55:14.919589Z"
    }
   },
   "outputs": [],
   "source": [
    "# eval\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../results/promting_T5\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdde02df-d06e-42bd-ae6d-ab0c95f5547d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-20T14:55:14.920701Z",
     "iopub.status.idle": "2024-02-20T14:55:14.920941Z",
     "shell.execute_reply": "2024-02-20T14:55:14.920832Z",
     "shell.execute_reply.started": "2024-02-20T14:55:14.920820Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    eval_dataset=tokenized_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca4f132-7639-456a-b14d-2d1ffc12cbd2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-20T14:55:14.922100Z",
     "iopub.status.idle": "2024-02-20T14:55:14.922334Z",
     "shell.execute_reply": "2024-02-20T14:55:14.922226Z",
     "shell.execute_reply.started": "2024-02-20T14:55:14.922214Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluation_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8328a28-ded9-4587-9e34-f6399f33d99e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334feb75-5fba-40ce-bfbf-d18792de42bd",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-20T14:55:14.923091Z",
     "iopub.status.idle": "2024-02-20T14:55:14.923334Z",
     "shell.execute_reply": "2024-02-20T14:55:14.923220Z",
     "shell.execute_reply.started": "2024-02-20T14:55:14.923208Z"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b597592-92e2-4d31-ae5f-f1f60af6763d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-20T14:55:14.923904Z",
     "iopub.status.idle": "2024-02-20T14:55:14.936260Z",
     "shell.execute_reply": "2024-02-20T14:55:14.924029Z",
     "shell.execute_reply.started": "2024-02-20T14:55:14.924018Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_similarity_sentence(sentences, top_k=5):\n",
    "    embeddings = model.encode(sentences)\n",
    "    cos_sim = util.cos_sim(embeddings, embeddings)\n",
    "    values, indices = cos_sim.topk(top_k+1)\n",
    "    top_k_indices = {}\n",
    "    for index in tqdm(range(len(indices))):\n",
    "        data = {\n",
    "            'top_index':indices[index][1:].tolist(),\n",
    "            'score':values[index][1:].tolist()\n",
    "        }\n",
    "        top_k_indices[index] = data\n",
    "    return top_k_indices\n",
    "\n",
    "\n",
    "def preprocess_prompt(examples, top_k_indices, corpus, labels, id2label):\n",
    "    sentences = examples[\"text\"]\n",
    "    prompts = []\n",
    "    for index, _ in enumerate(zip(sentences)):\n",
    "        prompt = 'Here are examples of texts and their sentiments'\n",
    "        top_indexs = top_k_indices[index]['top_index']\n",
    "        for top_index in top_indexs:\n",
    "            top_sentence = corpus[top_index]\n",
    "            top_label = id2label[str(labels[top_index])]\n",
    "            prompt = \" \".join(\n",
    "                [\n",
    "                    prompt,\n",
    "                    \". Text: \", \n",
    "                    top_sentence,\n",
    "                    \". Sentiment: \",\n",
    "                    top_label\n",
    "                ]\n",
    "            )\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    examples[\"prompt\"] = prompts\n",
    "    return examples\n",
    "\n",
    "def postprocess_text(predictions, labels):\n",
    "    predictions = [prediction.strip() for prediction in predictions]\n",
    "    labels = [label2id[label.strip()] for label in labels]\n",
    "\n",
    "    for idx in range(len(predictions)):\n",
    "        if predictions[idx] in label2id:\n",
    "           predictions[idx] = label2id[predictions[idx]]\n",
    "        else:\n",
    "            predictions[idx] = '-100'\n",
    "    return predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bb1ed7-c200-468b-9077-74bf77d1e940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch-2.1.0-py310",
   "language": "python",
   "name": "torch_py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
